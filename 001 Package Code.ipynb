{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95b4cc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from joblib import load\n",
    "from nltk.corpus import stopwords\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba6f4610",
   "metadata": {},
   "outputs": [],
   "source": [
    "#included_fields = load(os.path.join(os.path.dirname(__file__), 'data', 'included_fields.joblib'))\n",
    "#jurisdictions = load(os.path.join(os.path.dirname(__file__), 'data', 'jurisdictions.joblib'))\n",
    "#groups = load(os.path.join(os.path.dirname(__file__), 'data', 'groups.joblib'))\n",
    "#clf_field_names = load(os.path.join(os.path.dirname(__file__), 'data', 'clf_field_names.joblib'))\n",
    "\n",
    "included_fields = load('lib/data/included_fields.joblib')\n",
    "jurisdictions = load('lib/data/jurisdictions.joblib')\n",
    "groups = load('lib/data/groups.joblib')\n",
    "clf_field_names = load('lib/data/clf_field_names.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d5eee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "nlp = spacy.load('en_core_web_lg') # this takes a while to loadimport os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b19ff03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reCase(text):\n",
    "    # a quick and dirty way to pull words out of\n",
    "    # snake_case, camelCase and the like.\n",
    "    output = re.sub(\"(\\w|\\d)(_|-)(\\w|\\d)\",\"\\\\1 \\\\3\",text.strip())\n",
    "    output = re.sub(\"([a-z])([A-Z]|\\d)\",\"\\\\1 \\\\2\",output)\n",
    "    output = re.sub(\"(\\d)([A-Z]|[a-z])\",\"\\\\1 \\\\2\",output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "651e536d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asdasd asdsa'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reCase(\"asdasd_asdsa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9214ce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_norm_field(text):\n",
    "    # Takes an auto-generated form field name and uses\n",
    "    # regex to convert it into an Assembly Line standard field.\n",
    "    # See https://suffolklitlab.org/docassemble-AssemblyLine-documentation/docs/label_variables/\n",
    "\n",
    "    regex_list = [\n",
    "\n",
    "        # Personal info\n",
    "        ## Name & Bio\n",
    "        [\"^((My|Full( legal)?) )?Name$\",\"users1_name\"],\n",
    "        [\"^(Typed or )?Printed Name\\s?\\d*$\",\"users1_name\"],\n",
    "        [\"^(DOB|Date of Birth|Birthday)$\",\"users1_birthdate\"],\n",
    "        ## Address\n",
    "        [\"^(Street )?Address$\",\"users1_address_line_one\"],\n",
    "        [\"^City State Zip$\",\"users1_address_line_two\"],\n",
    "        [\"^City$\",\"users1_address_city\"],\n",
    "        [\"^State$\",\"users1_address_state\"],\n",
    "        [\"^Zip( Code)?$\",\"users1_address_zip\"],\n",
    "        ## Contact\n",
    "        [\"^(Phone|Telephone)$\",\"users1_phone_number\"],\n",
    "        [\"^Email( Adress)$\",\"users1_email\"],\n",
    "\n",
    "        # Parties\n",
    "        [\"^plaintiff\\(?s?\\)?$\",\"plantiff1_name\"],\n",
    "        [\"^defendant\\(?s?\\)?$\",\"defendant1_name\"],\n",
    "        [\"^petitioner\\(?s?\\)?$\",\"petitioners1_name\"],\n",
    "        [\"^respondent\\(?s?\\)?$\",\"respondents1_name\"],\n",
    "\n",
    "        # Court info\n",
    "        [\"^(Court\\s)?Case\\s?(No|Number)?\\s?A?$\",\"docket_number\"],\n",
    "        [\"^File\\s?(No|Number)?\\s?A?$\",\"docket_number\"],\n",
    "\n",
    "        # Form info\n",
    "        [\"^(Signature|Sign( here)?)\\s?\\d*$\",\"users1_signature\"],\n",
    "        [\"^Date\\s?\\d*$\",\"signature_date\"],\n",
    "    ]\n",
    "\n",
    "    for regex in regex_list:\n",
    "        text = re.sub(regex[0],regex[1],text, flags=re.IGNORECASE)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c501beaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'users1_name'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_norm_field(\"Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cd237ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat_field(text,max_length=30):\n",
    "    # h/t https://towardsdatascience.com/nlp-building-a-summariser-68e0c19e3a93\n",
    "\n",
    "    #print(text)\n",
    "\n",
    "    orig_title = text.lower()\n",
    "    orig_title = re.sub(\"[^a-zA-Z]+\",\" \",orig_title)\n",
    "    orig_title_words = orig_title.split()\n",
    "\n",
    "    deduped_sentence = []\n",
    "    for word in orig_title_words:\n",
    "        if word not in deduped_sentence:\n",
    "            deduped_sentence.append(word)\n",
    "\n",
    "    filtered_sentence = [w for w in deduped_sentence if not w.lower() in stop_words]\n",
    "\n",
    "    filtered_title_words = filtered_sentence\n",
    "\n",
    "    characters = len(' '.join(filtered_title_words))\n",
    "\n",
    "    if characters > 0:\n",
    "\n",
    "        words = len(filtered_title_words)\n",
    "        av_word_len = math.ceil(len(' '.join(filtered_title_words))/len(filtered_title_words))\n",
    "        x_words = math.floor((max_length)/av_word_len)\n",
    "\n",
    "\n",
    "        sim_mat = np.zeros([len(filtered_title_words),len(filtered_title_words)])# will populate it with cosine_similarity values\n",
    "        # for each word compared to other\n",
    "        for i in range(len(filtered_title_words)):\n",
    "            for j in range(len(filtered_title_words)):\n",
    "                if i != j:\n",
    "                    sim_mat[i][j] = cosine_similarity(nlp(filtered_title_words[i]).vector.reshape(1,300), nlp(filtered_title_words[j]).vector.reshape(1,300))[0,0]\n",
    "\n",
    "        try:\n",
    "            nx_graph = nx.from_numpy_array(sim_mat)\n",
    "            scores = nx.pagerank(nx_graph)# print final values of words\n",
    "            sorted_scores = sorted(scores.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "            if x_words > len(scores):\n",
    "                x_words=len(scores)\n",
    "\n",
    "            i = 0\n",
    "            new_title = \"\"\n",
    "            for x in filtered_title_words:\n",
    "                #print(scores[i],sorted_scores[x_words][1])\n",
    "                if scores[i] >= sorted_scores[x_words-1][1]:\n",
    "                    if len(new_title)>0: new_title+=\"_\"\n",
    "                    new_title += x\n",
    "                i+=1\n",
    "\n",
    "            return new_title\n",
    "        except:\n",
    "            return '_'.join(filtered_title_words)\n",
    "    else:\n",
    "        if re.search(\"^(\\d+)$\", text):\n",
    "            return \"unknown\"\n",
    "        else:\n",
    "            return re.sub(\"\\s+\",\"_\",text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45d12d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'name_field_fill'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reformat_field(\"this is a name field where you fill out your name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4d66685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(text):\n",
    "    output = nlp(str(text)).vector\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f6f434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(row):\n",
    "    try:\n",
    "        matrix = row.reshape(1,-1).astype(np.float64)\n",
    "        return normalize(matrix, axis=1, norm='l1')[0]\n",
    "    except Exception as e:\n",
    "        print(\"===================\")\n",
    "        print(\"Error: \",e)\n",
    "        print(\"===================\")\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ef93286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_name(jur,group,n,per,last_field,this_field):\n",
    "\n",
    "    # Add hard coded conversions maybe by calling a function\n",
    "    # if returns 0 then fail over to ML or otherway around poor prob -> check hard-coded\n",
    "\n",
    "    if this_field not in included_fields:\n",
    "        this_field = reCase(this_field)\n",
    "\n",
    "        out_put = regex_norm_field(this_field)\n",
    "        conf = 1.0\n",
    "\n",
    "        if out_put==this_field:\n",
    "            params = []\n",
    "            for item in jurisdictions:\n",
    "                if jur== item:\n",
    "                    params.append(1)\n",
    "                else:\n",
    "                    params.append(0)\n",
    "            for item in groups:\n",
    "                if group== item:\n",
    "                    params.append(1)\n",
    "                else:\n",
    "                    params.append(0)\n",
    "            params.append(n)\n",
    "            params.append(per)\n",
    "            for vec in norm(vectorize(this_field)):\n",
    "                params.append(vec)\n",
    "            #for vec in norm(vectorize(last_field)):\n",
    "            #    params.append(vec)\n",
    "\n",
    "            for item in included_fields:\n",
    "                if last_field==item:\n",
    "                    params.append(1)\n",
    "                else:\n",
    "                    params.append(0)\n",
    "\n",
    "            pred = clf_field_names.predict([params])\n",
    "            prob = clf_field_names.predict_proba([params])\n",
    "\n",
    "            conf = prob[0].tolist()[prob[0].tolist().index(max(prob[0].tolist()))]\n",
    "            out_put = pred[0]\n",
    "\n",
    "    else:\n",
    "        out_put = this_field\n",
    "        conf = 1\n",
    "\n",
    "    if out_put in included_fields:\n",
    "        if conf >= 0:\n",
    "            return \"*\"+out_put,conf #+\"| was <i>%s</i> (%.2f conf)\"%(this_field,conf) #, conf\n",
    "        else:\n",
    "            return reformat_field(this_field),conf #+\"| was <i>%s</i> (%.2f conf)\"%(this_field,conf) #, conf\n",
    "    else:\n",
    "        return reformat_field(this_field),conf #+\"| was <i>%s</i> (%.2f conf)\"%(this_field,conf) #, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09541a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('case_thing', 0.43)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_name(\"UT\",None,2,0.3,\"null\",\"Case thing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "173816e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_screens(fields=[],damping=0.7):\n",
    "    # Takes in a list (fields) and returns a suggested screen grouping\n",
    "    # Set damping to value >= 0.5 or < 1 to tune how related screens should be\n",
    "\n",
    "    vec_mat = np.zeros([len(fields),300])\n",
    "    for i in range(len(fields)):\n",
    "        vec_mat[i] = [nlp(reCase(fields[i])).vector][0]\n",
    "\n",
    "    # create model\n",
    "    model = AffinityPropagation(damping=damping,random_state=4)\n",
    "    # fit the model\n",
    "    model.fit(vec_mat)\n",
    "    # assign a cluster to each example\n",
    "    yhat = model.predict(vec_mat)\n",
    "    # retrieve unique clusters\n",
    "    clusters = unique(yhat)\n",
    "\n",
    "    screens = {}\n",
    "    #sim = np.zeros([5,300])\n",
    "    i=0\n",
    "    for cluster in clusters:\n",
    "        this_screen = where(yhat == cluster)[0]\n",
    "        vars = []\n",
    "        j=0\n",
    "        for screen in this_screen:\n",
    "            #sim[screen]=vec_mat[screen] # use this spot to add up vectors for compare to list\n",
    "            vars.append(fields[screen])\n",
    "            j+=1\n",
    "        screens[\"screen_%s\"%i]=vars\n",
    "        i+=1\n",
    "\n",
    "    return screens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58b5ce59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'screen_0': ['user', 'name', 'address'], 'screen_1': ['street']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_screens([\"user\",\"name\",\"address\",\"street\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b00e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
